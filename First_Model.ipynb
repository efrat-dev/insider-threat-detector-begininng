{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1kicHpD1RLPFKPK8xfQUh7ecy2exzULRk",
      "authorship_tag": "ABX9TyNUB5Caxmt8e2NxW5bDCwVf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/efrat-dev/insider-threat-detector/blob/main/First_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import roc_auc_score, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "def load_and_preprocess_data(filepath):\n",
        "    \"\"\"Load and preprocess the real data\"\"\"\n",
        "    print(\"Loading data...\")\n",
        "\n",
        "    # Try to load the CSV file\n",
        "    try:\n",
        "        df = pd.read_csv('/content/drive/MyDrive/processed_data.csv')\n",
        "        # df = pd.read_csv(filepath, on_bad_lines='skip')\n",
        "        print(f\"Loaded data with shape: {df.shape}\")\n",
        "        print(f\"Columns: {df.columns.tolist()}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Basic preprocessing\n",
        "    print(\"Preprocessing data...\")\n",
        "\n",
        "    # Find feature columns (exclude employee_id, date, is_malicious)\n",
        "    exclude_cols = ['employee_id', 'is_malicious']\n",
        "    if 'date' in df.columns:\n",
        "        exclude_cols.append('date')\n",
        "\n",
        "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
        "    print(f\"Found {len(feature_cols)} feature columns\")\n",
        "\n",
        "    # Handle categorical columns\n",
        "    processed_df = df.copy()\n",
        "    for col in feature_cols:\n",
        "        if processed_df[col].dtype == 'object':\n",
        "            le = LabelEncoder()\n",
        "            processed_df[col] = le.fit_transform(processed_df[col].astype(str))\n",
        "        else:\n",
        "            processed_df[col] = pd.to_numeric(processed_df[col], errors='coerce')\n",
        "            processed_df[col].fillna(processed_df[col].mean(), inplace=True)\n",
        "\n",
        "    return processed_df, feature_cols\n",
        "\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, df, feature_cols, seq_len=180, scaler=None):\n",
        "        self.seq_len = seq_len\n",
        "        self.feature_cols = feature_cols\n",
        "        self.samples = []\n",
        "        self.labels = []\n",
        "\n",
        "        print(f\"Processing {len(df['employee_id'].unique())} employees...\")\n",
        "\n",
        "        for emp_id in df['employee_id'].unique():\n",
        "            emp_df = df[df['employee_id'] == emp_id]\n",
        "\n",
        "            # Sort by date if available\n",
        "            if 'date' in emp_df.columns:\n",
        "                emp_df = emp_df.sort_values('date')\n",
        "\n",
        "            features = emp_df[feature_cols].values\n",
        "            label = emp_df['is_malicious'].iloc[0]\n",
        "\n",
        "            # Skip if no data\n",
        "            if len(features) == 0:\n",
        "                continue\n",
        "\n",
        "            # Pad or truncate to seq_len\n",
        "            if len(features) >= seq_len:\n",
        "                features = features[-seq_len:]  # Take last seq_len days\n",
        "            else:\n",
        "                # Pad with last known values\n",
        "                padding = np.tile(features[-1], (seq_len - len(features), 1))\n",
        "                features = np.vstack([features, padding])\n",
        "\n",
        "            if scaler:\n",
        "                features = scaler.transform(features)\n",
        "\n",
        "            self.samples.append(torch.tensor(features, dtype=torch.float32))\n",
        "            self.labels.append(torch.tensor(label, dtype=torch.float32))\n",
        "\n",
        "        print(f\"Created {len(self.samples)} samples\")\n",
        "        pos_samples = sum(self.labels)\n",
        "        print(f\"Positive samples: {pos_samples}, Negative samples: {len(self.labels) - pos_samples}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx], self.labels[idx]\n",
        "\n",
        "class SimpleLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128):\n",
        "        super(SimpleLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, dropout=0.2)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        out = self.dropout(lstm_out[:, -1, :])  # Take last time step\n",
        "        return self.fc(out).squeeze()\n",
        "\n",
        "def main():\n",
        "    print(\"=== Simple Pipeline Test with Real Data ===\")\n",
        "\n",
        "    # Load your real data\n",
        "    df, feature_cols = load_and_preprocess_data('processed_data.csv')\n",
        "\n",
        "    if df is None:\n",
        "        print(\"Failed to load data!\")\n",
        "        return None, None\n",
        "\n",
        "    # Use all features\n",
        "    print(f\"Using all {len(feature_cols)} features\")\n",
        "\n",
        "    # Split employees\n",
        "    employee_ids = df['employee_id'].unique()\n",
        "    employee_labels = df.groupby('employee_id')['is_malicious'].first()\n",
        "\n",
        "    train_ids, test_ids = train_test_split(\n",
        "        employee_ids, test_size=0.3,\n",
        "        stratify=employee_labels, random_state=42\n",
        "    )\n",
        "\n",
        "    train_df = df[df['employee_id'].isin(train_ids)]\n",
        "    test_df = df[df['employee_id'].isin(test_ids)]\n",
        "\n",
        "    print(f\"Train employees: {len(train_ids)}, Test employees: {len(test_ids)}\")\n",
        "\n",
        "    # Fit scaler on training data\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(train_df[feature_cols])\n",
        "\n",
        "    # Create datasets (full 180 days sequence)\n",
        "    train_dataset = SimpleDataset(train_df, feature_cols, seq_len=180, scaler=scaler)\n",
        "    test_dataset = SimpleDataset(test_df, feature_cols, seq_len=180, scaler=scaler)\n",
        "\n",
        "    if len(train_dataset) == 0:\n",
        "        print(\"No training samples found!\")\n",
        "        return None, None\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    print(f\"Train samples: {len(train_dataset)}, Test samples: {len(test_dataset)}\")\n",
        "\n",
        "    # Initialize model\n",
        "    model = SimpleLSTM(input_dim=len(feature_cols))\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Quick training (20 epochs for better results)\n",
        "    print(\"\\n=== Training ===\")\n",
        "    model.train()\n",
        "    for epoch in range(20):\n",
        "        total_loss = 0\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Loss = {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    # Evaluate\n",
        "    print(\"\\n=== Evaluation ===\")\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in test_loader:\n",
        "            outputs = model(batch_x)\n",
        "            probs = torch.sigmoid(outputs)\n",
        "            preds = (probs > 0.5).float()\n",
        "\n",
        "            all_preds.extend(preds.numpy())\n",
        "            all_probs.extend(probs.numpy())\n",
        "            all_labels.extend(batch_y.numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    if len(set(all_labels)) > 1:  # Check if we have both classes\n",
        "        auc_score = roc_auc_score(all_labels, all_probs)\n",
        "        print(f\"AUC Score: {auc_score:.4f}\")\n",
        "    else:\n",
        "        print(\"Only one class found in test set\")\n",
        "        auc_score = 0.0\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(all_labels, all_preds))\n",
        "\n",
        "    print(\"\\n=== Pipeline Test Complete ===\")\n",
        "    print(\"✓ Real data loading works\")\n",
        "    print(\"✓ Data preprocessing works\")\n",
        "    print(\"✓ Model training works\")\n",
        "    print(\"✓ Evaluation works\")\n",
        "    print(f\"✓ Final AUC: {auc_score:.4f}\")\n",
        "\n",
        "    return model, auc_score\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, auc_score = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I13o7nfy4O9i",
        "outputId": "2c0e3e0f-7d1b-4266-d9be-8d62fc9d6c7d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Simple Pipeline Test with Real Data ===\n",
            "Loading data...\n",
            "Loaded data with shape: (180000, 161)\n",
            "Columns: ['employee_id', 'employee_seniority_years', 'is_malicious', 'num_print_commands', 'num_bw_prints', 'ratio_color_prints', 'day', 'day_of_year', 'entry_minute', 'exit_hour', 'exit_minute', 'print_intensity', 'print_efficiency', 'avg_presence_per_entry', 'behavioral_risk_advanced', 'time_consistency_score', 'media_risk_score', 'employee_department_label', 'employee_department_target', 'employee_department_freq', 'employee_campus_cat_Campus A', 'employee_campus_cat_Campus B', 'employee_campus_cat_Campus C', 'employee_campus_label', 'employee_position_label', 'employee_position_target', 'employee_position_freq', 'employee_classification_cat_2', 'employee_classification_cat_3', 'employee_classification_label', 'employee_origin_country_label', 'employee_origin_country_target', 'employee_origin_country_freq', 'print_campuses_cat_0', 'num_exits_cat_1', 'num_exits_label', 'weekday_label', 'quarter_cat_1', 'quarter_cat_2', 'quarter_label', 'is_weekend_binary', 'is_quarter_end_binary', 'season_cat_spring', 'season_cat_winter', 'season_label', 'entry_hour_label', 'work_duration_category_cat_long', 'work_duration_category_label', 'print_mobility_score_cat_1.0', 'print_mobility_score_label', 'access_frequency_cat_2', 'access_frequency_label', 'access_pattern_label', 'unusual_work_pattern_binary', 'schedule_stability_binary', 'employee_id_log', 'employee_id_above_mean', 'employee_id_quartile', 'employee_seniority_years_log', 'employee_seniority_years_above_mean', 'employee_seniority_years_quartile', 'num_print_commands_log', 'num_print_commands_above_mean', 'num_print_commands_quartile', 'total_printed_pages_log', 'total_printed_pages_quartile', 'num_bw_prints_quartile', 'ratio_color_prints_above_mean', 'ratio_color_prints_quartile', 'total_presence_minutes_quartile', 'day_log', 'day_above_mean', 'day_quartile', 'day_of_year_log', 'day_of_year_above_mean', 'day_of_year_quartile', 'week_of_year_quartile', 'entry_minute_above_mean', 'entry_minute_quartile', 'entry_time_numeric_quartile', 'exit_hour_above_mean', 'exit_hour_quartile', 'exit_minute_above_mean', 'exit_minute_quartile', 'exit_time_numeric_quartile', 'work_duration_hours_quartile', 'avg_pages_per_print_quartile', 'print_intensity_log', 'print_intensity_quartile', 'bw_print_preference_quartile', 'print_efficiency_log', 'print_efficiency_quartile', 'avg_presence_per_entry_above_mean', 'avg_presence_per_entry_quartile', 'presence_intensity_log', 'presence_intensity_quartile', 'behavioral_risk_advanced_log', 'behavioral_risk_advanced_above_mean', 'behavioral_risk_advanced_quartile', 'time_consistency_score_quartile', 'media_risk_score_log', 'media_risk_score_quartile', 'poly_behavioral_risk_advanced access_risk_profile_log', 'poly_behavioral_risk_advanced access_risk_profile_above_mean', 'poly_behavioral_risk_advanced access_risk_profile_quartile', 'employee_department_label_log', 'employee_department_label_above_mean', 'employee_department_label_quartile', 'employee_department_target_log', 'employee_department_target_above_mean', 'employee_department_target_quartile', 'employee_department_freq_log', 'employee_department_freq_above_mean', 'employee_department_freq_quartile', 'employee_campus_label_log', 'employee_campus_label_above_mean', 'employee_position_label_log', 'employee_position_label_above_mean', 'employee_position_label_quartile', 'employee_position_target_log', 'employee_position_target_sqrt', 'employee_position_target_above_mean', 'employee_position_target_quartile', 'employee_position_freq_log', 'employee_position_freq_quartile', 'employee_classification_label_log', 'employee_classification_label_above_mean', 'employee_classification_label_quartile', 'employee_origin_country_label_log', 'employee_origin_country_label_above_mean', 'employee_origin_country_label_quartile', 'employee_origin_country_target_log', 'employee_origin_country_target_above_mean', 'employee_origin_country_target_quartile', 'employee_origin_country_freq_log', 'employee_origin_country_freq_quartile', 'print_campuses_label_log', 'print_campuses_label_quartile', 'country_name_label_quartile', 'country_name_target_quartile', 'num_entries_label_quartile', 'num_exits_label_log', 'num_exits_label_quartile', 'num_unique_campus_label_quartile', 'month_label_quartile', 'weekday_label_above_mean', 'weekday_label_quartile', 'is_quarter_end_binary_log', 'season_label_log', 'entry_hour_label_log', 'entry_hour_label_above_mean', 'entry_hour_label_quartile', 'work_duration_category_label_log', 'work_duration_category_label_quartile', 'print_mobility_score_label_log', 'print_mobility_score_label_quartile', 'access_frequency_label_quartile', 'access_pattern_label_above_mean', 'access_pattern_label_quartile', 'unusual_work_pattern_binary_log', 'schedule_stability_binary_log']\n",
            "Preprocessing data...\n",
            "Found 159 feature columns\n",
            "Using all 159 features\n",
            "Train employees: 700, Test employees: 300\n",
            "Processing 700 employees...\n",
            "Created 700 samples\n",
            "Positive samples: 35.0, Negative samples: 665.0\n",
            "Processing 300 employees...\n",
            "Created 300 samples\n",
            "Positive samples: 15.0, Negative samples: 285.0\n",
            "Train samples: 700, Test samples: 300\n",
            "\n",
            "=== Training ===\n",
            "Epoch 1: Loss = 0.2935\n",
            "Epoch 2: Loss = 0.1305\n",
            "Epoch 3: Loss = 0.0716\n",
            "Epoch 4: Loss = 0.0186\n",
            "Epoch 5: Loss = 0.0062\n",
            "Epoch 6: Loss = 0.0029\n",
            "Epoch 7: Loss = 0.0020\n",
            "Epoch 8: Loss = 0.0017\n",
            "Epoch 9: Loss = 0.0013\n",
            "Epoch 10: Loss = 0.0010\n",
            "Epoch 11: Loss = 0.0008\n",
            "Epoch 12: Loss = 0.0007\n",
            "Epoch 13: Loss = 0.0007\n",
            "Epoch 14: Loss = 0.0006\n",
            "Epoch 15: Loss = 0.0005\n",
            "Epoch 16: Loss = 0.0004\n",
            "Epoch 17: Loss = 0.0003\n",
            "Epoch 18: Loss = 0.0003\n",
            "Epoch 19: Loss = 0.0003\n",
            "Epoch 20: Loss = 0.0003\n",
            "\n",
            "=== Evaluation ===\n",
            "AUC Score: 1.0000\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00       285\n",
            "         1.0       1.00      0.93      0.97        15\n",
            "\n",
            "    accuracy                           1.00       300\n",
            "   macro avg       1.00      0.97      0.98       300\n",
            "weighted avg       1.00      1.00      1.00       300\n",
            "\n",
            "\n",
            "=== Pipeline Test Complete ===\n",
            "✓ Real data loading works\n",
            "✓ Data preprocessing works\n",
            "✓ Model training works\n",
            "✓ Evaluation works\n",
            "✓ Final AUC: 1.0000\n"
          ]
        }
      ]
    }
  ]
}